import sys
from itertools import chain
import keras
from keras.utils import to_categorical
from keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt
import numpy
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import tldextract
def parsed_url(url):
    subdomain, domain, domain_suffix = ('<empty>' if extracted == '' else extracted for extracted in tldextract.extract(url))
    return [subdomain, domain, domain_suffix]
def updt(total, progress):
    barLength, status = 20, ""
    progress = float(progress) / float(total)
    if progress >= 1.:
        progress, status = 1, "\r\n"
    block = int(round(barLength * progress))
    text = "\r[{}] {:.0f}% {}".format("#"*block+"-"*(barLength-block) , round(progress * 100, 0),
        status)
    sys.stdout.write(text)
    sys.stdout.flush()
def extract_url(data):
    extract_url_data = [parsed_url(url) for url in data['url']]
    extract_url_data = pd.DataFrame(extract_url_data, columns=['subdomain', 'domain', 'domain_suffix'])
    data = data.reset_index(drop=True)
    data = pd.concat([data, extract_url_data], axis=1)
    return data
def wordprocessing(texts):
    tokenizer  = Tokenizer(num_words = 10000)
    tokenizer.fit_on_texts(texts)
    sequences =  tokenizer.texts_to_sequences(texts)
    pading = pad_sequences(sequences, maxlen=10000)
    return pading
def make_text_index_dic(_text):
    word_set = set()
    for text in _text:
        for word in text:
            word_set.add(word)
    word_dic = {'word':0}
    i = 1
    for word in word_set:
        word_dic.update({word:i})
        i = i + 1
    return word_dic
def word2numbers(Aword):
    letters=[]
    numbers=[]
    for i in Aword:
        if(i!=' '):
            letters.append(i)
    for j in letters:
        numbers.append(word_list.get(j))
    return numbers
def preprocess(url):
    lst=[]
    subdomain=word2numbers(parsed_url(url)[0])
    domain   =word2numbers(parsed_url(url)[1])
    suffix   =word2numbers(parsed_url(url)[2])
    zeros    =numpy.zeros(256)
    lst.extend([domain,suffix,zeros])
    lst=numpy.asarray(tf.keras.preprocessing.sequence.pad_sequences(numpy.asarray([numpy.array(xi) for xi in list(lst)]),padding='post'))
    lst=lst/36
    lst=lst[0:2]
    lst=lst.reshape(1,16,16,2)
    print(lst.shape)
    return lst
def predict_url(preprocessed_data):
    pred=numpy.argmax(model.predict(lst))
    if(pred==0):
        print("Malicious")
    else:
        print("Safe Web site")
data=pd.read_csv("data.csv")
print("Importing Dataset")
np_data  =data['url'].to_numpy()
np_labels=data['label'].to_numpy()
subdomain,domain,suffix,a=[],[],[],[]
l_labels=[]
col_names=['subdomain', 'domain', 'suffix']
pd_data  = pd.DataFrame(columns = col_names)
Ra=int(len(np_data)*0.001)
for i in range(Ra):
    a=parsed_url((np_data[i]))
    subdomain,domain,suffix=a[0],a[1],a[2]
    l_labels.append(np_labels[i])
    pd_data.loc[len(pd_data)]=[subdomain,domain,suffix]  
    updt(Ra, i + 1)
l_labels.append(0)
print('✓ Imported Dataset')
np_data  =pd_data.to_numpy()
np_labels=numpy.asarray(l_labels)
print("✓ Generated Data and labels")
word_bag = list(chain(np_data[:,0],np_data[:,1],np_data[:,2])) 
word_list=make_text_index_dic(word_bag)
words_lots=[]
count=0
for i in range(len(word_bag)):
    if(word_bag[i]=='<empty>'):
        count=count+1
    else:
        words_lots.append(word_bag[i])
count,max_letter,s_sum=0,0,0
for i in range(len(words_lots)):
    if(len(words_lots[i])<max_letter):
        count=count+1
    else:
        max_letter=len(words_lots[i])
    s_sum=s_sum+len(words_lots[i])

print('✓ cleaned data')
numb_subdomain,numb_domain,numb_suffix=[],[],[]
for i in range(len(np_data[:,0])):
    numb_subdomain.append(word2numbers(np_data[i,0]))
    numb_domain.append(word2numbers(np_data[i,1]))
    numb_suffix.append(word2numbers(np_data[i,2]))

numb_subdomain.append( numpy.zeros(256))
numb_domain.append( numpy.zeros(256))
numb_suffix.append( numpy.zeros(256))

numb_subdomain=tf.keras.preprocessing.sequence.pad_sequences(numpy.asarray([numpy.array(xi) for xi in numb_subdomain]),padding='post')
numb_domain   =tf.keras.preprocessing.sequence.pad_sequences(numpy.asarray([numpy.array(xi) for xi in numb_domain]),padding='post')
numb_suffix   =tf.keras.preprocessing.sequence.pad_sequences(numpy.asarray([numpy.array(xi) for xi in numb_suffix]),padding='post')
print('✓ Preprocessing Data completed')
col_names =  ['subdomain', 'domain', 'suffix']
processed_data  = pd.DataFrame(columns = col_names)
loc=[]
for i in range(len(numb_domain)):
    loc.append([numb_subdomain[i],numb_domain[i]])
loc=numpy.asarray(loc)
loc=loc/36
loc=loc.reshape((Ra+1),16,16,2)
lab=[]
for i in range(len(l_labels)):
    if(l_labels=='bad'):
        lab.append(0)
    else:
        lab.append(1)
Y = to_categorical(lab)
print('✓ preprocessing the labels and one hotencoding')
model = models.Sequential()
model.add(layers.Conv2D(64, (8, 8), activation='relu', input_shape=(16, 16, 2)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(8, activation='relu'))
model.add(layers.Dense(2))
print("✓ DNN generated")
print("  ")
print("  ")
model.compile(loss= 'mean_squared_error' , optimizer= 'adam' , metrics=['accuracy'])
print("Model Summary")
model.summary()
model.fit(loc,Y, epochs=10,batch_size= 10,verbose=1)
test_mse = model.evaluate(loc, Y, verbose=0)
print("loss",test_mse[0],"accuracy",test_mse[1])
Testing_URL=sys.argv[1]
lst=preprocess(Testing_URL)
predict_url(lst)
